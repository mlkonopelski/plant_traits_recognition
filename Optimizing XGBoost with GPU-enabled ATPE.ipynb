{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":65626,"databundleVersionId":7929795,"sourceType":"competition"}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This is just for clear outputs of cells. Shouldn't be used during development\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T19:11:02.561187Z","iopub.execute_input":"2024-04-06T19:11:02.562046Z","iopub.status.idle":"2024-04-06T19:11:02.566512Z","shell.execute_reply.started":"2024-04-06T19:11:02.562013Z","shell.execute_reply":"2024-04-06T19:11:02.565375Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# GOAL\n\nOne approach to this challange is to train boosting algorithm on just tabular data. My choice is XGBoost but in very simmilar fashion you can try LightGBM or CatBoost or even stacked 3 those models together (which I will do in separate notebook). \n\nXGBoost has following number of parameters (you can find more details using `help(xgboost.XGBRegressor)`): ","metadata":{}},{"cell_type":"code","source":"#  |      n_estimators : Optional[int]\n#  |          Number of gradient boosted trees.  Equivalent to number of boosting\n#  |          rounds.\n#  |  \n#  |      max_depth :  Optional[int]\n#  |          Maximum tree depth for base learners.\n#  |      max_leaves :\n#  |          Maximum number of leaves; 0 indicates no limit.\n#  |      max_bin :\n#  |          If using histogram-based algorithm, maximum number of bins per feature\n#  |      grow_policy :\n#  |          Tree growing policy. 0: favor splitting at nodes closest to the node, i.e. grow\n#  |          depth-wise. 1: favor splitting at nodes with highest loss change.\n#  |      learning_rate : Optional[float]\n#  |          Boosting learning rate (xgb's \"eta\")\n#  |      verbosity : Optional[int]\n#  |          The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n#  |      objective : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], typing.Tuple[numpy.ndarray, numpy.ndarray]], NoneType]\n#  |          Specify the learning task and the corresponding learning objective or\n#  |          a custom objective function to be used (see note below).\n#  |      booster: Optional[str]\n#  |          Specify which booster to use: gbtree, gblinear or dart.\n#  |      tree_method: Optional[str]\n#  |          Specify which tree method to use.  Default to auto.  If this parameter is set to\n#  |          default, XGBoost will choose the most conservative option available.  It's\n#  |          recommended to study this option from the parameters document :doc:`tree method\n#  |          </treemethod>`\n#  |      n_jobs : Optional[int]\n#  |          Number of parallel threads used to run xgboost.  When used with other\n#  |          Scikit-Learn algorithms like grid search, you may choose which algorithm to\n#  |          parallelize and balance the threads.  Creating thread contention will\n#  |          significantly slow down both algorithms.\n#  |      gamma : Optional[float]\n#  |          (min_split_loss) Minimum loss reduction required to make a further partition on a\n#  |          leaf node of the tree.\n#  |      min_child_weight : Optional[float]\n#  |          Minimum sum of instance weight(hessian) needed in a child.\n#  |      max_delta_step : Optional[float]\n#  |          Maximum delta step we allow each tree's weight estimation to be.\n#  |      subsample : Optional[float]\n#  |          Subsample ratio of the training instance.\n#  |      sampling_method :\n#  |          Sampling method. Used only by the GPU version of ``hist`` tree method.\n#  |            - ``uniform``: select random training instances uniformly.\n#  |            - ``gradient_based`` select random training instances with higher probability\n#  |              when the gradient and hessian are larger. (cf. CatBoost)\n#  |      colsample_bytree : Optional[float]\n#  |          Subsample ratio of columns when constructing each tree.\n#  |      colsample_bylevel : Optional[float]\n#  |          Subsample ratio of columns for each level.\n#  |      colsample_bynode : Optional[float]\n#  |          Subsample ratio of columns for each split.\n#  |      reg_alpha : Optional[float]\n#  |          L1 regularization term on weights (xgb's alpha).\n#  |      reg_lambda : Optional[float]\n#  |          L2 regularization term on weights (xgb's lambda).\n#  |      scale_pos_weight : Optional[float]\n#  |          Balancing of positive and negative weights.\n#  |      base_score : Optional[float]\n#  |          The initial prediction score of all instances, global bias.\n#  |      random_state : Optional[Union[numpy.random.RandomState, int]]\n#  |          Random number seed\n#  |      missing : float, default np.nan\n#  |          Value in the data which needs to be present as a missing value.\n#  |      num_parallel_tree: Optional[int]\n#  |          Used for boosting random forest.\n#  |      monotone_constraints : Optional[Union[Dict[str, int], str]]\n#  |          Constraint of variable monotonicity.  See :doc:`tutorial </tutorials/monotonic>`\n#  |          for more information.\n#  |      interaction_constraints : Optional[Union[str, List[Tuple[str]]]]\n#  |          Constraints for interaction representing permitted interactions.  The\n#  |          constraints must be specified in the form of a nested list, e.g. ``[[0, 1], [2,\n#  |          3, 4]]``, where each inner list is a group of indices of features that are\n#  |          allowed to interact with each other.  See :doc:`tutorial\n#  |          </tutorials/feature_interaction_constraint>` for more information\n#  |      importance_type: Optional[str]\n#  |          The feature importance type for the feature_importances\\_ property:\n#  |  \n#  |          * For tree model, it's either \"gain\", \"weight\", \"cover\", \"total_gain\" or\n#  |            \"total_cover\".\n#  |          * For linear model, only \"weight\" is defined and it's the normalized coefficients\n#  |            without bias.\n#  |  \n#  |      device : Optional[str]\n#  |  \n#  |          .. versionadded:: 2.0.0\n#  |  \n#  |          Device ordinal, available options are `cpu`, `cuda`, and `gpu`.\n#  |  \n#  |      validate_parameters : Optional[bool]\n#  |  \n#  |          Give warnings for unknown parameter.\n#  |  \n#  |      enable_categorical : bool\n#  |  \n#  |          .. versionadded:: 1.5.0\n#  |  \n#  |          .. note:: This parameter is experimental\n#  |  \n#  |          Experimental support for categorical data.  When enabled, cudf/pandas.DataFrame\n#  |          should be used to specify categorical data type.  Also, JSON/UBJSON\n#  |          serialization format is required.\n#  |  \n#  |      feature_types : Optional[FeatureTypes]\n#  |  \n#  |          .. versionadded:: 1.7.0\n#  |  \n#  |          Used for specifying feature types without constructing a dataframe. See\n#  |          :py:class:`DMatrix` for details.\n#  |  \n#  |      max_cat_to_onehot : Optional[int]\n#  |  \n#  |          .. versionadded:: 1.6.0\n#  |  \n#  |          .. note:: This parameter is experimental\n#  |  \n#  |          A threshold for deciding whether XGBoost should use one-hot encoding based split\n#  |          for categorical data.  When number of categories is lesser than the threshold\n#  |          then one-hot encoding is chosen, otherwise the categories will be partitioned\n#  |          into children nodes. Also, `enable_categorical` needs to be set to have\n#  |          categorical feature support. See :doc:`Categorical Data\n#  |          </tutorials/categorical>` and :ref:`cat-param` for details.\n#  |  \n#  |      max_cat_threshold : Optional[int]\n#  |  \n#  |          .. versionadded:: 1.7.0\n#  |  \n#  |          .. note:: This parameter is experimental\n#  |  \n#  |          Maximum number of categories considered for each split. Used only by\n#  |          partition-based splits for preventing over-fitting. Also, `enable_categorical`\n#  |          needs to be set to have categorical feature support. See :doc:`Categorical Data\n#  |          </tutorials/categorical>` and :ref:`cat-param` for details.\n#  |  \n#  |      multi_strategy : Optional[str]\n#  |  \n#  |          .. versionadded:: 2.0.0\n#  |  \n#  |          .. note:: This parameter is working-in-progress.\n#  |  \n#  |          The strategy used for training multi-target models, including multi-target\n#  |          regression and multi-class classification. See :doc:`/tutorials/multioutput` for\n#  |          more information.\n#  |  \n#  |          - ``one_output_per_tree``: One model for each target.\n#  |          - ``multi_output_tree``:  Use multi-target trees.\n#  |  \n#  |      eval_metric : Optional[Union[str, List[str], Callable]]\n#  |  \n#  |          .. versionadded:: 1.6.0\n#  |  \n#  |          Metric used for monitoring the training result and early stopping.  It can be a\n#  |          string or list of strings as names of predefined metric in XGBoost (See\n#  |          doc/parameter.rst), one of the metrics in :py:mod:`sklearn.metrics`, or any other\n#  |          user defined metric that looks like `sklearn.metrics`.\n#  |  \n#  |          If custom objective is also provided, then custom metric should implement the\n#  |          corresponding reverse link function.\n#  |  \n#  |          Unlike the `scoring` parameter commonly used in scikit-learn, when a callable\n#  |          object is provided, it's assumed to be a cost function and by default XGBoost will\n#  |          minimize the result during early stopping.\n#  |  \n#  |          For advanced usage on Early stopping like directly choosing to maximize instead of\n#  |          minimize, see :py:obj:`xgboost.callback.EarlyStopping`.\n#  |  \n#  |          See :doc:`Custom Objective and Evaluation Metric </tutorials/custom_metric_obj>`\n#  |          for more.\n#  |  \n#  |          .. note::\n#  |  \n#  |               This parameter replaces `eval_metric` in :py:meth:`fit` method.  The old\n#  |               one receives un-transformed prediction regardless of whether custom\n#  |               objective is being used.\n#  |  \n#  |          .. code-block:: python\n#  |  \n#  |              from sklearn.datasets import load_diabetes\n#  |              from sklearn.metrics import mean_absolute_error\n#  |              X, y = load_diabetes(return_X_y=True)\n#  |              reg = xgb.XGBRegressor(\n#  |                  tree_method=\"hist\",\n#  |                  eval_metric=mean_absolute_error,\n#  |              )\n#  |              reg.fit(X, y, eval_set=[(X, y)])\n#  |  \n#  |      early_stopping_rounds : Optional[int]\n#  |  \n#  |          .. versionadded:: 1.6.0\n#  |  \n#  |          - Activates early stopping. Validation metric needs to improve at least once in\n#  |            every **early_stopping_rounds** round(s) to continue training.  Requires at\n#  |            least one item in **eval_set** in :py:meth:`fit`.\n#  |  \n#  |          - If early stopping occurs, the model will have two additional attributes:\n#  |            :py:attr:`best_score` and :py:attr:`best_iteration`. These are used by the\n#  |            :py:meth:`predict` and :py:meth:`apply` methods to determine the optimal\n#  |            number of trees during inference. If users want to access the full model\n#  |            (including trees built after early stopping), they can specify the\n#  |            `iteration_range` in these inference methods. In addition, other utilities\n#  |            like model plotting can also use the entire model.\n#  |  \n#  |          - If you prefer to discard the trees after `best_iteration`, consider using the\n#  |            callback function :py:class:`xgboost.callback.EarlyStopping`.\n#  |  \n#  |          - If there's more than one item in **eval_set**, the last entry will be used for\n#  |            early stopping.  If there's more than one metric in **eval_metric**, the last\n#  |            metric will be used for early stopping.\n#  |  \n#  |          .. note::\n#  |  \n#  |              This parameter replaces `early_stopping_rounds` in :py:meth:`fit` method.\n#  |  \n#  |      callbacks : Optional[List[TrainingCallback]]\n#  |          List of callback functions that are applied at end of each iteration.\n#  |          It is possible to use predefined callbacks by using\n#  |          :ref:`Callback API <callback_api>`.\n#  |  \n#  |          .. note::\n#  |  \n#  |             States in callback are not preserved during training, which means callback\n#  |             objects can not be reused for multiple training sessions without\n#  |             reinitialization or deepcopy.\n#  |  \n#  |          .. code-block:: python\n#  |  \n#  |              for params in parameters_grid:\n#  |                  # be sure to (re)initialize the callbacks before each run\n#  |                  callbacks = [xgb.callback.LearningRateScheduler(custom_rates)]\n#  |                  reg = xgboost.XGBRegressor(**params, callbacks=callbacks)\n#  |                  reg.fit(X, y)\n#  |  \n#  |      kwargs : dict, optional\n#  |          Keyword arguments for XGBoost Booster object.  Full documentation of parameters\n#  |          can be found :doc:`here </parameter>`.\n#  |          Attempting to set a parameter via the constructor args and \\*\\*kwargs\n#  |          dict simultaneously will result in a TypeError.\n#  |  \n#  |          .. note:: \\*\\*kwargs unsupported by scikit-learn\n#  |  \n#  |              \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee\n#  |              that parameters passed via this argument will interact properly\n#  |              with scikit-learn.\n#  |  \n#  |          .. note::  Custom objective function\n#  |  \n#  |              A custom objective function can be provided for the ``objective``\n#  |              parameter. In this case, it should have the signature\n#  |              ``objective(y_true, y_pred) -> grad, hess``:\n#  |  \n#  |              y_true: array_like of shape [n_samples]\n#  |                  The target values\n#  |              y_pred: array_like of shape [n_samples]\n#  |                  The predicted values\n#  |  \n#  |              grad: array_like of shape [n_samples]\n#  |                  The value of the gradient for each sample point.\n#  |              hess: array_like of shape [n_samples]\n#  |                  The value of the second derivative for each sample point","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2024-04-06T19:11:02.594909Z","iopub.execute_input":"2024-04-06T19:11:02.595314Z","iopub.status.idle":"2024-04-06T19:11:02.611112Z","shell.execute_reply.started":"2024-04-06T19:11:02.595287Z","shell.execute_reply":"2024-04-06T19:11:02.610162Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"While default parameters works pretty well there is always room for improvment.  \nHere comes a optimization library called `hyperopt`: [DOCS](https://hyperopt.github.io/hyperopt/)","metadata":{}},{"cell_type":"code","source":"HYPEROPT_RUNS = 250","metadata":{"execution":{"iopub.status.busy":"2024-04-06T19:11:02.613070Z","iopub.execute_input":"2024-04-06T19:11:02.613466Z","iopub.status.idle":"2024-04-06T19:11:02.623412Z","shell.execute_reply.started":"2024-04-06T19:11:02.613431Z","shell.execute_reply":"2024-04-06T19:11:02.622583Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# DATA","metadata":{}},{"cell_type":"code","source":"import pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-04-06T19:11:02.624585Z","iopub.execute_input":"2024-04-06T19:11:02.624842Z","iopub.status.idle":"2024-04-06T19:11:02.632717Z","shell.execute_reply.started":"2024-04-06T19:11:02.624819Z","shell.execute_reply":"2024-04-06T19:11:02.631847Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/planttraits2024/train.csv')\ntest_df = pd.read_csv('/kaggle/input/planttraits2024/test.csv')","metadata":{"execution":{"iopub.status.busy":"2024-04-06T19:11:02.635197Z","iopub.execute_input":"2024-04-06T19:11:02.635912Z","iopub.status.idle":"2024-04-06T19:11:04.241836Z","shell.execute_reply.started":"2024-04-06T19:11:02.635880Z","shell.execute_reply":"2024-04-06T19:11:04.241017Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"submission_cols = ['X4_mean', 'X11_mean', 'X18_mean', 'X50_mean', 'X26_mean', 'X3112_mean']\ntest_cols = list(set(train_df.columns).difference(test_df.columns))\ntrain_cols = [col for col in train_df.columns if col not in test_cols]","metadata":{"execution":{"iopub.status.busy":"2024-04-06T19:11:04.243352Z","iopub.execute_input":"2024-04-06T19:11:04.243670Z","iopub.status.idle":"2024-04-06T19:11:04.249290Z","shell.execute_reply.started":"2024-04-06T19:11:04.243644Z","shell.execute_reply":"2024-04-06T19:11:04.248204Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"pd.read_csv('/kaggle/input/planttraits2024/target_name_meta.tsv', sep='\\t')","metadata":{"execution":{"iopub.status.busy":"2024-04-06T19:11:04.250399Z","iopub.execute_input":"2024-04-06T19:11:04.250798Z","iopub.status.idle":"2024-04-06T19:11:04.266401Z","shell.execute_reply.started":"2024-04-06T19:11:04.250761Z","shell.execute_reply":"2024-04-06T19:11:04.265466Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"  trait_ID                                        trait_name \n0       X4  Stem specific density (SSD) or wood density (s...\n1      X11  Leaf area per leaf dry mass (specific leaf are...\n2      X18                                      Plant height \n3      X26                                     Seed dry mass \n4      X50           Leaf nitrogen (N) content per leaf area \n5    X3112  Leaf area (in case of compound leaves: leaf, u...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>trait_ID</th>\n      <th>trait_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>X4</td>\n      <td>Stem specific density (SSD) or wood density (s...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>X11</td>\n      <td>Leaf area per leaf dry mass (specific leaf are...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>X18</td>\n      <td>Plant height</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>X26</td>\n      <td>Seed dry mass</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>X50</td>\n      <td>Leaf nitrogen (N) content per leaf area</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>X3112</td>\n      <td>Leaf area (in case of compound leaves: leaf, u...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"## Pandas Profiling\n\nI'm using Pandas Profiling tool to automatically do full statistical report of each column.  \nIf doing properly EDA was goal of this Notebook I would certainly go over traning columns in-depth by removing `, minimal=True` argument and apply some processing to outliers/nan etc. ","metadata":{}},{"cell_type":"code","source":"# !pip install pandas-profiling[notebook]\n# from ydata_profiling import ProfileReport","metadata":{"execution":{"iopub.status.busy":"2024-04-06T19:11:04.268814Z","iopub.execute_input":"2024-04-06T19:11:04.269562Z","iopub.status.idle":"2024-04-06T19:11:04.273094Z","shell.execute_reply.started":"2024-04-06T19:11:04.269525Z","shell.execute_reply":"2024-04-06T19:11:04.272199Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# %%time\n# train_profile = ProfileReport(train_df, title=\"Profiling Report\", minimal=True)\n# train_profile","metadata":{"execution":{"iopub.status.busy":"2024-04-06T19:11:04.274314Z","iopub.execute_input":"2024-04-06T19:11:04.274658Z","iopub.status.idle":"2024-04-06T19:11:04.282148Z","shell.execute_reply.started":"2024-04-06T19:11:04.274627Z","shell.execute_reply":"2024-04-06T19:11:04.281156Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# %%time\n# test_profile = ProfileReport(test_df, title=\"Profiling Report\", minimal=True)\n# test_profile","metadata":{"execution":{"iopub.status.busy":"2024-04-06T19:11:04.283189Z","iopub.execute_input":"2024-04-06T19:11:04.283491Z","iopub.status.idle":"2024-04-06T19:11:04.291638Z","shell.execute_reply.started":"2024-04-06T19:11:04.283466Z","shell.execute_reply":"2024-04-06T19:11:04.290807Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"## Clear Outliers\nAs many pointed out before me this dataset has few huge outliers that will certainly skew your model.  \nEasiest and quite accurate solution is to just remove them from training dataframe using some kind of rule. It can be either: \n1. Based on treshold in case you have a good expertise in this topic. \n1. Based on quantile. I remove any sample from 1% lowest and 1% highest values. This is quite harsh because I might removing samples that are correct if NO outliers are present but in this case it still works well.\n\nI used Pandas profiling again to look for max/min of each column but also sd. ","metadata":{}},{"cell_type":"code","source":"# target_profile = ProfileReport(train_df[submission_cols], title=\"Profiling Report\", minimal=True)\n# target_profile","metadata":{"execution":{"iopub.status.busy":"2024-04-06T19:11:04.292690Z","iopub.execute_input":"2024-04-06T19:11:04.292950Z","iopub.status.idle":"2024-04-06T19:11:04.301239Z","shell.execute_reply.started":"2024-04-06T19:11:04.292928Z","shell.execute_reply":"2024-04-06T19:11:04.300465Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"df = train_df.copy()\n\nprint(f'Before cleaning df has {df.shape[0]} rows')\nfor col in submission_cols:\n    q_low = df[col].quantile(0.01)\n    q_hi  = df[col].quantile(0.99)\n    rows = df.shape[0]\n    df = df[(df[col] < q_hi) & (df[col] > q_low)]\n    print(f'\\tCleaning: {col} removed {rows - df.shape[0]} rows')\nprint(f'After cleaning df has {df.shape[0]} rows')","metadata":{"execution":{"iopub.status.busy":"2024-04-06T19:11:04.302229Z","iopub.execute_input":"2024-04-06T19:11:04.302504Z","iopub.status.idle":"2024-04-06T19:11:04.468298Z","shell.execute_reply.started":"2024-04-06T19:11:04.302471Z","shell.execute_reply":"2024-04-06T19:11:04.467397Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Before cleaning df has 55489 rows\n\tCleaning: X4_mean removed 1113 rows\n\tCleaning: X11_mean removed 1090 rows\n\tCleaning: X18_mean removed 1067 rows\n\tCleaning: X50_mean removed 1047 rows\n\tCleaning: X26_mean removed 1026 rows\n\tCleaning: X3112_mean removed 1007 rows\nAfter cleaning df has 49139 rows\n","output_type":"stream"}]},{"cell_type":"markdown","source":"I recommend splitting train to X,y since many libraries prefer to read data in this format.  \nI keep the original df because RAM utilization is not an issue here. ","metadata":{}},{"cell_type":"code","source":"train_df = df\ny = train_df[submission_cols]\nX = train_df.drop(columns=['id'] + test_cols)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T19:11:04.469571Z","iopub.execute_input":"2024-04-06T19:11:04.469936Z","iopub.status.idle":"2024-04-06T19:11:04.496463Z","shell.execute_reply.started":"2024-04-06T19:11:04.469902Z","shell.execute_reply":"2024-04-06T19:11:04.495542Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# Check again if this method has cleaned the target \n# target_profile = ProfileReport(train_df[submission_cols], title=\"Profiling Report\", minimal=True)\n# target_profile","metadata":{"execution":{"iopub.status.busy":"2024-04-06T19:11:04.500980Z","iopub.execute_input":"2024-04-06T19:11:04.501651Z","iopub.status.idle":"2024-04-06T19:11:04.505339Z","shell.execute_reply.started":"2024-04-06T19:11:04.501623Z","shell.execute_reply":"2024-04-06T19:11:04.504321Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"## KFold split\nTesting models on just one validation set sometimes might be \"overfit\" towards certain random split. A better solution is to calculate average metrics over each e.g. 20% subset. However this gives a lot of overhead to optimization process because each set of hyperparameters have to be trained 5 times instead of just 1. This also means when tiem is castrained you will validate 5 times smaller hyperparameters space. ","metadata":{}},{"cell_type":"code","source":"from sklearn import model_selection","metadata":{"execution":{"iopub.status.busy":"2024-04-06T19:11:04.506513Z","iopub.execute_input":"2024-04-06T19:11:04.506838Z","iopub.status.idle":"2024-04-06T19:11:04.514288Z","shell.execute_reply.started":"2024-04-06T19:11:04.506804Z","shell.execute_reply":"2024-04-06T19:11:04.513512Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"SPLITS = 5\n\ntrain_df['kfold']= -1\ntrain_df = train_df.sample(frac=1).reset_index(drop=True)\ntrain_df.loc[:, \"bins\"] = pd.cut(\n train_df[\"X4_mean\"], bins=19, labels=False\n )\nkf = model_selection.StratifiedKFold(n_splits=SPLITS)\nfor f, (t_, v_) in enumerate(kf.split(X=train_df, y=train_df.bins.values)):\n    train_df.loc[v_, 'kfold'] = f\ntrain_df = train_df.drop(\"bins\", axis=1)\n\ny = train_df[submission_cols + ['kfold']]\nX = train_df.drop(columns=['id'] + test_cols)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T19:11:04.515381Z","iopub.execute_input":"2024-04-06T19:11:04.515780Z","iopub.status.idle":"2024-04-06T19:11:04.686057Z","shell.execute_reply.started":"2024-04-06T19:11:04.515748Z","shell.execute_reply":"2024-04-06T19:11:04.685074Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"# hyperopt\nIn summary `fmin` works very easily as we need to define 3 major components:\n1. Parameters space that we will look for, e.g. we want to validate models between 100 and 500 trees as we assume that >500 will give no benefits besides overfitting.\n1. Function to optimize in our case it's training and validating model by calculating `-r2` score \n1. Optimization technique. In comparison to e.g. GridSearch we don't test every combination of parameters but:\n    1. During N first runs randomly select parameters and calculate results to create a multidimensional \"map\"\n    1. During next N runs. Find local minimum by changing parameters in serrounding of best parameters set from 1. using e.g. TPE algorithm: [PAPER](https://proceedings.neurips.cc/paper_files/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf). It's usefull to limit the optimization runs using:\n        1. `early stopping` when metric (like loss) deosn't improve in k runs. \n        2. `timeout` when we just want to run test for fixed time.","metadata":{}},{"cell_type":"code","source":"from hyperopt import fmin\nhelp(fmin)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T19:11:04.687444Z","iopub.execute_input":"2024-04-06T19:11:04.687743Z","iopub.status.idle":"2024-04-06T19:11:05.332163Z","shell.execute_reply.started":"2024-04-06T19:11:04.687718Z","shell.execute_reply":"2024-04-06T19:11:05.331260Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Help on function fmin in module hyperopt.fmin:\n\nfmin(fn, space, algo=None, max_evals=None, timeout=None, loss_threshold=None, trials=None, rstate=None, allow_trials_fmin=True, pass_expr_memo_ctrl=None, catch_eval_exceptions=False, verbose=True, return_argmin=True, points_to_evaluate=None, max_queue_len=1, show_progressbar=True, early_stop_fn=None, trials_save_file='')\n    Minimize a function over a hyperparameter space.\n    \n    More realistically: *explore* a function over a hyperparameter space\n    according to a given algorithm, allowing up to a certain number of\n    function evaluations.  As points are explored, they are accumulated in\n    `trials`\n    \n    \n    Parameters\n    ----------\n    \n    fn : callable (trial point -> loss)\n        This function will be called with a value generated from `space`\n        as the first and possibly only argument.  It can return either\n        a scalar-valued loss, or a dictionary.  A returned dictionary must\n        contain a 'status' key with a value from `STATUS_STRINGS`, must\n        contain a 'loss' key if the status is `STATUS_OK`. Particular\n        optimization algorithms may look for other keys as well.  An\n        optional sub-dictionary associated with an 'attachments' key will\n        be removed by fmin its contents will be available via\n        `trials.trial_attachments`. The rest (usually all) of the returned\n        dictionary will be stored and available later as some 'result'\n        sub-dictionary within `trials.trials`.\n    \n    space : hyperopt.pyll.Apply node or \"annotated\"\n        The set of possible arguments to `fn` is the set of objects\n        that could be created with non-zero probability by drawing randomly\n        from this stochastic program involving involving hp_<xxx> nodes\n        (see `hyperopt.hp` and `hyperopt.pyll_utils`).\n        If set to \"annotated\", will read space using type hint in fn. Ex:\n        (`def fn(x: hp.uniform(\"x\", -1, 1)): return x`)\n    \n    algo : search algorithm\n        This object, such as `hyperopt.rand.suggest` and\n        `hyperopt.tpe.suggest` provides logic for sequential search of the\n        hyperparameter space.\n    \n    max_evals : int\n        Allow up to this many function evaluations before returning.\n    \n    timeout : None or int, default None\n        Limits search time by parametrized number of seconds.\n        If None, then the search process has no time constraint.\n    \n    loss_threshold : None or double, default None\n        Limits search time when minimal loss reduced to certain amount.\n        If None, then the search process has no constraint on the loss,\n        and will stop based on other parameters, e.g. `max_evals`, `timeout`\n    \n    trials : None or base.Trials (or subclass)\n        Storage for completed, ongoing, and scheduled evaluation points.  If\n        None, then a temporary `base.Trials` instance will be created.  If\n        a trials object, then that trials object will be affected by\n        side-effect of this call.\n    \n    rstate : numpy.random.Generator, default numpy.random or `$HYPEROPT_FMIN_SEED`\n        Each call to `algo` requires a seed value, which should be different\n        on each call. This object is used to draw these seeds via `randint`.\n        The default rstate is\n        `numpy.random.default_rng(int(env['HYPEROPT_FMIN_SEED']))`\n        if the `HYPEROPT_FMIN_SEED` environment variable is set to a non-empty\n        string, otherwise np.random is used in whatever state it is in.\n    \n    verbose : bool\n        Print out some information to stdout during search. If False, disable\n            progress bar irrespectively of show_progressbar argument\n    \n    allow_trials_fmin : bool, default True\n        If the `trials` argument\n    \n    pass_expr_memo_ctrl : bool, default False\n        If set to True, `fn` will be called in a different more low-level\n        way: it will receive raw hyperparameters, a partially-populated\n        `memo`, and a Ctrl object for communication with this Trials\n        object.\n    \n    return_argmin : bool, default True\n        If set to False, this function returns nothing, which can be useful\n        for example if it is expected that `len(trials)` may be zero after\n        fmin, and therefore `trials.argmin` would be undefined.\n    \n    points_to_evaluate : list, default None\n        Only works if trials=None. If points_to_evaluate equals None then the\n        trials are evaluated normally. If list of dicts is passed then\n        given points are evaluated before optimisation starts, so the overall\n        number of optimisation steps is len(points_to_evaluate) + max_evals.\n        Elements of this list must be in a form of a dictionary with variable\n        names as keys and variable values as dict values. Example\n        points_to_evaluate value is [{'x': 0.0, 'y': 0.0}, {'x': 1.0, 'y': 2.0}]\n    \n    max_queue_len : integer, default 1\n        Sets the queue length generated in the dictionary or trials. Increasing this\n        value helps to slightly speed up parallel simulatulations which sometimes lag\n        on suggesting a new trial.\n    \n    show_progressbar : bool or context manager, default True (or False is verbose is False).\n        Show a progressbar. See `hyperopt.progress` for customizing progress reporting.\n    \n    early_stop_fn: callable ((result, *args) -> (Boolean, *args)).\n        Called after every run with the result of the run and the values returned by the function previously.\n        Stop the search if the function return true.\n        Default None.\n    \n    trials_save_file: str, default \"\"\n        Optional file name to save the trials object to every iteration.\n        If specified and the file already exists, will load from this file when\n        trials=None instead of creating a new base.Trials object\n    \n    Returns\n    -------\n    \n    argmin : dictionary\n        If return_argmin is True returns `trials.argmin` which is a dictionary.  Otherwise\n        this function  returns the result of `hyperopt.space_eval(space, trails.argmin)` if there\n        were successfull trails. This object shares the same structure as the space passed.\n        If there were no successfull trails, it returns None.\n\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch #just for chekccing if cuda is available\n\nimport cupy as cp\n\nfrom xgboost import XGBRegressor\n\nfrom hyperopt import hp, tpe, atpe, Trials, STATUS_OK, STATUS_FAIL\nfrom hyperopt.pyll.base import scope\nfrom hyperopt.early_stop import no_progress_loss\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn import ensemble\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\n\nimport numpy as np\nfrom functools import partial\n\nimport warnings \nnp.warnings = warnings","metadata":{"execution":{"iopub.status.busy":"2024-04-06T19:37:49.349240Z","iopub.execute_input":"2024-04-06T19:37:49.349994Z","iopub.status.idle":"2024-04-06T19:37:49.356342Z","shell.execute_reply.started":"2024-04-06T19:37:49.349961Z","shell.execute_reply":"2024-04-06T19:37:49.355244Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nDEVICE","metadata":{"execution":{"iopub.status.busy":"2024-04-06T19:11:08.730738Z","iopub.execute_input":"2024-04-06T19:11:08.731192Z","iopub.status.idle":"2024-04-06T19:11:08.770219Z","shell.execute_reply.started":"2024-04-06T19:11:08.731168Z","shell.execute_reply":"2024-04-06T19:11:08.769111Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"markdown","source":"### Important detail!\nI would recommend NOT calculating R2 as mean of individual split r2. But instead keep all y_preds and y_test in separate table and calculate mean of all predictions. It's an easy fix","metadata":{}},{"cell_type":"code","source":"def optimize(params,x,y):\n    # Occacionally the choice of parameters might led to failed training. \n    # We don't want to interupt the optimization process in such case\n    # In this case I have particularly problem with it when with existing setting \n    # I added multi_strategy=multi_output_tree\n    try:\n        model= XGBRegressor(random_state=0,\n                            multi_strategy=\"one_output_per_tree\",\n                            tree_method = \"hist\",\n                            booster = \"gbtree\",\n                            objective = \"reg:squarederror\",\n                            eval_metric = \"rmse\",\n                            device=DEVICE, \n                            **params)\n        # RMSE = []\n        R2 = []\n        for i in range(SPLITS):\n            X_train=x[x.kfold != i].drop(columns=['kfold'])\n            y_train=y[y.kfold !=i].drop(columns=['kfold'])\n            X_test=x[x.kfold == i].drop(columns=['kfold'])\n            y_test=y[y.kfold==i].drop(columns=['kfold'])\n            model.fit(cp.array(X_train), cp.array(y_train)) # I'm using cupy to move data from CPU to GPU\n            y_preds = model.predict(cp.array(X_test))\n            r2 = r2_score(cp.asnumpy(y_test), cp.asnumpy(y_preds))\n            # fold_ras = mean_squared_error(y_test, preds, squared=False)\n            # RMSE.append(fold_ras)\n            R2.append(r2)\n        score = np.mean(R2)\n        return {'loss': - score, 'status': STATUS_OK, 'accuracy': score}\n    except:\n        print(f'-------------------\\nPARAMS - which led to unstable failed model\\n{params}')\n        return {'loss': 9999, 'status': STATUS_FAIL, 'accuracy': - 9999} ","metadata":{"execution":{"iopub.status.busy":"2024-04-06T19:48:17.807476Z","iopub.execute_input":"2024-04-06T19:48:17.807836Z","iopub.status.idle":"2024-04-06T19:48:17.817568Z","shell.execute_reply.started":"2024-04-06T19:48:17.807810Z","shell.execute_reply":"2024-04-06T19:48:17.816664Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"### Important detail!\nWe have 6 columns to predict and XGBoost Regressor provides 2 useful strategies to handle multi output regression:\n1. one_output_per_tree - XGBoost builds one model for each target similar to sklearn meta estimators, with the added benefit of reusing data and other integrated features like SHAP. \n1. multi_output_tree - XGBoost can optionally build multi-output trees with the size of leaf equals to the number of targets when the tree method hist is used. The behavior can be controlled by the multi_strategy training parameter, which can take the value one_output_per_tree (the default) for building one model per-target or multi_output_tree for building multi-output trees.\n\nIt's worth exploring. Here link to [DOCS](https://xgboost.readthedocs.io/en/latest/tutorials/multioutput.html)","metadata":{}},{"cell_type":"code","source":"param_space = {\n \"learning_rate\": hp.uniform(\"learning_rate\", 0.001, 0.2),\n \"max_depth\": scope.int(hp.quniform(\"max_depth\", 3, 15, 1)),\n \"n_estimators\": scope.int(hp.quniform(\"n_estimators\", 50, 500, 1)),\n \"subsample\": hp.uniform(\"subsample\", 0.6, 0.98),\n \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.6, 0.98),\n \"colsample_bynode\": hp.uniform(\"colsample_bynode\", 0.6, 0.98),\n \"reg_alpha\": hp.uniform(\"reg_alpha\", 0, 15),\n \"reg_lambda\": hp.uniform(\"reg_lambda\", 0, 15),\n \"gamma\": hp.uniform(\"gamma\", 0, 15),\n \"min_child_weight\": hp.choice('min_child_weight', np.arange(0, 101).tolist()),\n }","metadata":{"execution":{"iopub.status.busy":"2024-04-06T19:48:22.217403Z","iopub.execute_input":"2024-04-06T19:48:22.218046Z","iopub.status.idle":"2024-04-06T19:48:22.225881Z","shell.execute_reply.started":"2024-04-06T19:48:22.218013Z","shell.execute_reply":"2024-04-06T19:48:22.224962Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":"There are 2 major algorithms for hyperopt optimization:\n1. TPE: `tpe.suggest`\n1. Adaptive TPE `atpe.sugges`\nHOwever I don't know the difference. Just that TPE enables to find ","metadata":{}},{"cell_type":"code","source":"optimization_function = partial(\n optimize,\n  x=X, y=y\n )\n\n\ntrials = Trials() \n\nhopt = fmin( \n fn=optimization_function,\n space=param_space,\n algo=atpe.suggest, #partial(tpe.suggest, n_startup_jobs=50), # we have large space to validate and it defaults to 20 random runs\n max_evals=HYPEROPT_RUNS,\n trials=trials,\n early_stop_fn=no_progress_loss(25)\n )","metadata":{"execution":{"iopub.status.busy":"2024-04-06T19:48:23.011311Z","iopub.execute_input":"2024-04-06T19:48:23.012114Z","iopub.status.idle":"2024-04-06T19:52:47.527213Z","shell.execute_reply.started":"2024-04-06T19:48:23.012083Z","shell.execute_reply":"2024-04-06T19:52:47.526248Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"100%|██████████| 5/5 [04:24<00:00, 52.90s/trial, best loss: -0.18705792407141678]\n","output_type":"stream"}]},{"cell_type":"code","source":"hopt","metadata":{"execution":{"iopub.status.busy":"2024-04-06T19:53:43.351154Z","iopub.execute_input":"2024-04-06T19:53:43.351532Z","iopub.status.idle":"2024-04-06T19:53:43.357714Z","shell.execute_reply.started":"2024-04-06T19:53:43.351503Z","shell.execute_reply":"2024-04-06T19:53:43.356702Z"},"trusted":true},"execution_count":69,"outputs":[{"execution_count":69,"output_type":"execute_result","data":{"text/plain":"{'colsample_bynode': 0.7152456813353711,\n 'colsample_bytree': 0.7627571379110261,\n 'gamma': 0.5199736188983278,\n 'learning_rate': 0.1464373047362146,\n 'max_depth': 7.0,\n 'min_child_weight': 2,\n 'n_estimators': 113.0,\n 'reg_alpha': 9.520494494793626,\n 'reg_lambda': 2.148294343109651,\n 'subsample': 0.7537201126074058}"},"metadata":{}}]},{"cell_type":"code","source":"final_model = XGBRegressor(device=DEVICE,\n                           # Those are FIXED parameters for this problem\n                            multi_strategy=\"one_output_per_tree\",\n                            tree_method = \"hist\",\n                            booster = \"gbtree\",\n                            objective = \"reg:squarederror\",\n                            eval_metric = \"rmse\",\n                           \n                           # Here are my optimized parameters\n                           learning_rate=hopt['learning_rate'],\n                           max_depth=int(hopt['max_depth']),\n                           n_estimators=int(hopt['n_estimators']),\n                           gamma= hopt['gamma'],\n                           reg_alpha=hopt['reg_alpha'],\n                           reg_lambda=hopt['reg_lambda'],\n                           subsample=hopt['subsample'],\n                           colsample_bynode= hopt['colsample_bynode'],\n                           colsample_bytree= hopt['colsample_bytree'],\n                           min_child_weight=hopt['min_child_weight'],\n                          )","metadata":{"execution":{"iopub.status.busy":"2024-04-06T19:53:46.196622Z","iopub.execute_input":"2024-04-06T19:53:46.197325Z","iopub.status.idle":"2024-04-06T19:53:46.203956Z","shell.execute_reply.started":"2024-04-06T19:53:46.197294Z","shell.execute_reply":"2024-04-06T19:53:46.202786Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"markdown","source":"# Final Validation\nHere I just want to check if model build on 90% of data has a simmilar r2 to hyper opt results. \nAdditionally just for the sake of presentation I fit and validate base XGBoost model for comparison","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X.drop(columns=['kfold']), y.drop(columns=['kfold']), test_size=0.1)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T19:53:47.966499Z","iopub.execute_input":"2024-04-06T19:53:47.966840Z","iopub.status.idle":"2024-04-06T19:53:48.037749Z","shell.execute_reply.started":"2024-04-06T19:53:47.966813Z","shell.execute_reply":"2024-04-06T19:53:48.036760Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"final_model.fit(X_train, y_train)\n\ny_preds = final_model.predict(X_test)\ny_preds = pd.DataFrame(y_preds)\ny_preds[y_preds < 0] = 0\n\nprint(f'Total R2: {r2_score(y_test, y_preds)}')\n\nfor i in range(6):\n    r2 = r2_score(y_test.iloc[:, i], y_preds.iloc[:, i])\n    print(f'\\t{submission_cols[i]} R2: {r2}')","metadata":{"execution":{"iopub.status.busy":"2024-04-06T19:53:48.389755Z","iopub.execute_input":"2024-04-06T19:53:48.390317Z","iopub.status.idle":"2024-04-06T19:53:52.973340Z","shell.execute_reply.started":"2024-04-06T19:53:48.390287Z","shell.execute_reply":"2024-04-06T19:53:52.972459Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"Total R2: 0.19295305963192\n\tX4_mean R2: 0.17478360158270878\n\tX11_mean R2: 0.21902053739133442\n\tX18_mean R2: 0.2841029158918813\n\tX50_mean R2: 0.14427002361764496\n\tX26_mean R2: 0.07708908537138692\n\tX3112_mean R2: 0.2584521939365657\n","output_type":"stream"}]},{"cell_type":"code","source":"default_model = XGBRegressor(multi_strategy=\"one_output_per_tree\",\n                            tree_method = \"hist\",\n                            booster = \"gbtree\",\n                            objective = \"reg:squarederror\",\n                            eval_metric = \"rmse\",\n                            random_state=0)\n\ndefault_model.fit(X_train, y_train)\n\ny_preds = default_model.predict(X_test)\ny_preds = pd.DataFrame(y_preds)\ny_preds[y_preds < 0] = 0\n\nfor i in range(6):\n    r2 = r2_score(y_test.iloc[:, i], y_preds.iloc[:, i])\n    print(f'{submission_cols[i]} R2: {r2}')","metadata":{"execution":{"iopub.status.busy":"2024-04-06T19:52:52.135101Z","iopub.execute_input":"2024-04-06T19:52:52.135523Z","iopub.status.idle":"2024-04-06T19:53:17.142708Z","shell.execute_reply.started":"2024-04-06T19:52:52.135488Z","shell.execute_reply":"2024-04-06T19:53:17.141800Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"X4_mean R2: 0.22134965301097687\nX11_mean R2: 0.1750409065377323\nX18_mean R2: 0.28732835978624505\nX50_mean R2: 0.13970645110257374\nX26_mean R2: 0.041247500885678745\nX3112_mean R2: 0.22611519783688894\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# FINAL PREDICTION\nThis is the last happy step. \n\nOur model is ready to be applied to test data.   \n\n\n### Importand detail!\nAs all my target values are positive floats I use additional condition because sometimes the prediction was negative: `preds_array[preds_array < 0] = 0`","metadata":{}},{"cell_type":"code","source":"preds = pd.DataFrame({'id': test_df['id']})\npreds_array = final_model.predict(test_df.drop(columns=['id']))\npreds_array[preds_array < 0] = 0\npreds['X4'] = preds_array[:, 0]\npreds['X11'] = preds_array[:, 1]\npreds['X18'] = preds_array[:, 2]\npreds['X50'] = preds_array[:, 3]\npreds['X26'] = preds_array[:, 4]\npreds['X3112'] = preds_array[:, 5]\npreds","metadata":{"execution":{"iopub.status.busy":"2024-04-06T19:53:17.144284Z","iopub.execute_input":"2024-04-06T19:53:17.144585Z","iopub.status.idle":"2024-04-06T19:53:17.234634Z","shell.execute_reply.started":"2024-04-06T19:53:17.144560Z","shell.execute_reply":"2024-04-06T19:53:17.233663Z"},"trusted":true},"execution_count":66,"outputs":[{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"             id        X4        X11       X18       X50         X26  \\\n0     201238668  0.563673   9.186352  3.669922  1.511492    7.919033   \n1     202310319  0.446374  17.951296  0.327758  1.407170   15.138796   \n2     202604412  0.457808  15.638722  1.652201  1.577334    5.158240   \n3     201353439  0.444903  20.544180  0.742931  1.254074    3.163327   \n4     195351745  0.519610   7.637208  0.057761  1.612624   10.767485   \n...         ...       ...        ...       ...       ...         ...   \n6540  195548469  0.645325   9.098532  2.951799  2.010241   16.642595   \n6541  199261251  0.555234  16.807568  6.737134  1.506822   73.620857   \n6542  203031744  0.472583  18.784073  2.126093  1.171336   11.332295   \n6543  197736382  0.444352  19.163837  0.991580  1.548013    6.426862   \n6544  202625693  0.557757  13.381124  8.655602  1.628984  169.110687   \n\n            X3112  \n0      591.096863  \n1      814.586914  \n2      804.832397  \n3      822.430420  \n4      613.946655  \n...           ...  \n6540   649.938477  \n6541  6212.021973  \n6542  2617.725098  \n6543  1411.909058  \n6544  1997.589722  \n\n[6545 rows x 7 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>X4</th>\n      <th>X11</th>\n      <th>X18</th>\n      <th>X50</th>\n      <th>X26</th>\n      <th>X3112</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>201238668</td>\n      <td>0.563673</td>\n      <td>9.186352</td>\n      <td>3.669922</td>\n      <td>1.511492</td>\n      <td>7.919033</td>\n      <td>591.096863</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>202310319</td>\n      <td>0.446374</td>\n      <td>17.951296</td>\n      <td>0.327758</td>\n      <td>1.407170</td>\n      <td>15.138796</td>\n      <td>814.586914</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>202604412</td>\n      <td>0.457808</td>\n      <td>15.638722</td>\n      <td>1.652201</td>\n      <td>1.577334</td>\n      <td>5.158240</td>\n      <td>804.832397</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>201353439</td>\n      <td>0.444903</td>\n      <td>20.544180</td>\n      <td>0.742931</td>\n      <td>1.254074</td>\n      <td>3.163327</td>\n      <td>822.430420</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>195351745</td>\n      <td>0.519610</td>\n      <td>7.637208</td>\n      <td>0.057761</td>\n      <td>1.612624</td>\n      <td>10.767485</td>\n      <td>613.946655</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6540</th>\n      <td>195548469</td>\n      <td>0.645325</td>\n      <td>9.098532</td>\n      <td>2.951799</td>\n      <td>2.010241</td>\n      <td>16.642595</td>\n      <td>649.938477</td>\n    </tr>\n    <tr>\n      <th>6541</th>\n      <td>199261251</td>\n      <td>0.555234</td>\n      <td>16.807568</td>\n      <td>6.737134</td>\n      <td>1.506822</td>\n      <td>73.620857</td>\n      <td>6212.021973</td>\n    </tr>\n    <tr>\n      <th>6542</th>\n      <td>203031744</td>\n      <td>0.472583</td>\n      <td>18.784073</td>\n      <td>2.126093</td>\n      <td>1.171336</td>\n      <td>11.332295</td>\n      <td>2617.725098</td>\n    </tr>\n    <tr>\n      <th>6543</th>\n      <td>197736382</td>\n      <td>0.444352</td>\n      <td>19.163837</td>\n      <td>0.991580</td>\n      <td>1.548013</td>\n      <td>6.426862</td>\n      <td>1411.909058</td>\n    </tr>\n    <tr>\n      <th>6544</th>\n      <td>202625693</td>\n      <td>0.557757</td>\n      <td>13.381124</td>\n      <td>8.655602</td>\n      <td>1.628984</td>\n      <td>169.110687</td>\n      <td>1997.589722</td>\n    </tr>\n  </tbody>\n</table>\n<p>6545 rows × 7 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"pd.read_csv('/kaggle/input/planttraits2024/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2024-04-06T19:55:28.715829Z","iopub.execute_input":"2024-04-06T19:55:28.716676Z","iopub.status.idle":"2024-04-06T19:55:28.731198Z","shell.execute_reply.started":"2024-04-06T19:55:28.716643Z","shell.execute_reply":"2024-04-06T19:55:28.730295Z"},"trusted":true},"execution_count":76,"outputs":[{"execution_count":76,"output_type":"execute_result","data":{"text/plain":"          id         X4        X11        X18        X50        X26      X3112\n0  201238668  21.228750  46.704747  31.430122  14.088638  24.208952  93.351051\n1  202310319   4.938497  71.644437  50.208404  99.503446  86.561125  20.325735\n2  202604412  34.268159  93.046483  74.556941  84.275746  85.713291  23.470287\n3  201353439  48.213879  84.687775  -1.442158  -9.852137  13.327949  -0.274766\n4  195351745  55.117501  87.046172  84.335483  79.218376  19.142174  11.294033","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>X4</th>\n      <th>X11</th>\n      <th>X18</th>\n      <th>X50</th>\n      <th>X26</th>\n      <th>X3112</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>201238668</td>\n      <td>21.228750</td>\n      <td>46.704747</td>\n      <td>31.430122</td>\n      <td>14.088638</td>\n      <td>24.208952</td>\n      <td>93.351051</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>202310319</td>\n      <td>4.938497</td>\n      <td>71.644437</td>\n      <td>50.208404</td>\n      <td>99.503446</td>\n      <td>86.561125</td>\n      <td>20.325735</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>202604412</td>\n      <td>34.268159</td>\n      <td>93.046483</td>\n      <td>74.556941</td>\n      <td>84.275746</td>\n      <td>85.713291</td>\n      <td>23.470287</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>201353439</td>\n      <td>48.213879</td>\n      <td>84.687775</td>\n      <td>-1.442158</td>\n      <td>-9.852137</td>\n      <td>13.327949</td>\n      <td>-0.274766</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>195351745</td>\n      <td>55.117501</td>\n      <td>87.046172</td>\n      <td>84.335483</td>\n      <td>79.218376</td>\n      <td>19.142174</td>\n      <td>11.294033</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# EXPORT TO CSV","metadata":{}},{"cell_type":"code","source":"preds.to_csv('preds.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-06T19:53:17.292671Z","iopub.status.idle":"2024-04-06T19:53:17.293049Z","shell.execute_reply.started":"2024-04-06T19:53:17.292842Z","shell.execute_reply":"2024-04-06T19:53:17.292856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_model.save_model('/kaggle/working/xgboost.json')","metadata":{"execution":{"iopub.status.busy":"2024-04-06T19:53:17.294603Z","iopub.status.idle":"2024-04-06T19:53:17.294936Z","shell.execute_reply.started":"2024-04-06T19:53:17.294775Z","shell.execute_reply":"2024-04-06T19:53:17.294793Z"},"trusted":true},"execution_count":null,"outputs":[]}]}